The dataset is first pre-processed to transform it into the format suitable to be used by the algorithm. This typically involves encoding and normalization. Sometimes, the dataset requires cleaning in terms of removing entries with missing data and duplicate entries, which is also performed during this phase. The pre-processed data is then divided randomly into two portions, the _training dataset_, and the _testing dataset_. Typically, the training dataset comprises almost 80% of the original dataset size and the remaining 20% forms testing dataset.